# 🚀 Twitter Data Pipeline with Airflow and Python

## Overview:

📊 I recently undertook a data engineering project, crafting a robust data pipeline to adeptly collect and analyze vast troves of Twitter data. Empowered by Python, Apache Airflow, and Amazon Web Services (AWS), the pipeline seamlessly extracted data from the Twitter API, underwent intricate transformations, deployed to an Apache Airflow instance on AWS EC2, and culminated with data storage on AWS S3 for in-depth analysis.

## 🗺️ Project Roadmap:

In this journey, a well-defined roadmap unfolded, beginning with the creation of a comprehensive architecture diagram. This visual masterpiece illuminated the intricate dance of data between components.

The spotlight then shifted to Apache Airflow, a symphony director for workflows. Crafting a directed acyclic graph (DAG) and choreographing individual tasks set the stage for the entire pipeline.

The pipeline's curtain call featured executing tasks, orchestrating the Twitter API setup, and securing the essential API key and secret. A flawless performance was ensured by deploying the code onto Apache Airflow.

## 🌐 Architecture:

![Pipeline Architecture](https://github.com/Kamu08/Twitter_Data_Pipeline/assets/87929852/536e5d1c-9e35-4a57-b030-845d7512c141)

## 💻 Technologies Used:

- **Python and Pandas**
- **Apache Airflow**
- **Amazon EC2**
- **Amazon S3**

## 🎓 What I Learned Through This Project:

🚀 This project was a masterclass in constructing a data pipeline, enriching my data engineering repertoire. Illuminating takeaways include:

- 🎭 Utilizing Apache Airflow to elegantly choreograph workflows.
- 🤖 Extracting Twitter API data, orchestrating Python and Pandas for analysis.
- 🔐 Navigating the nuances and boundaries of the Twitter API.
- ☁️ Mastering AWS EC2 deployment and S3 data storage—a data engineer's essential toolkit.

In summary, this project was a harmonious blend of challenge and reward, allowing me to apply data engineering concepts in a real-world context. The skills and insights gained are my compass as I chart a course in the dynamic realm of data engineering.
